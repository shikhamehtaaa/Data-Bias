# Data-Bias
For this project, I intended to explore bias in an existing natural language processing model, Google's Perspective API. During this process, I parsed a dataset of comments labeled for toxicity by manual reviewers, formulated a hypothesis regardining potential bias in the API, determined a threshold for toxicity, formulated a sample dataset to test my hypothesis, and collected and analyzed toxicity scores from my hypothesis tests. Through this experience, I familiarized myself with new Python libraries, learned more about programming and cleaning data, and engaged with Perspective API. 

In order to properly explore bias in Perspective API, I had to begin by importing various libraries. I knew that I would need some libraries for parsing, like BeautifulSoup, or for working with DataFrames, like Pandas. As I continued working with the given data, I noticed that I might need to translate the comments to English to get a better understanding of frequent words in toxic comments, so I attempted to use various language libraries until I determined that deep_translator's GoogleTranslator worked best. Additionally, to determine the most frequently occurring words relevant to toxicity in toxic comments, I downloaded nltk's stopwords. Afterwards, I created a DataFrame from the CSV file of sample labeled data. Next, I created a list from the column of comments in the DataFrame and translated it. Afterwards, I iterated through keys in a dictionary to create a list of comments marked as toxic. I used stopwords to remove irrelevant words from the list. Finally, I could iterate through the list to create a dictionary that pairs words with their frequencies. I repeated these steps for nontoxic comments as well so that I could compare frequently occurring words in the two categories. During this comparison, I noticed that there were many negative words with similar meanings that appeared in high frequency in both toxic and nontoxic comments. However, the longer word often appeared in higher frequency in the nontoxic comments, and the shorter word often appeared in high frequency in the toxic comments. The model may be biased towards the length of words because shorter words are frequently associated with slang, which might be more frequently marked as toxic than formal language. After formulating my hypothesis, I decided to get toxicity scores for toxic comments from the CSV file to determine a threshold value for toxicity. I found ten comments marked as toxic, and I documented their toxicity scores. Using a for loop, I found the average of their toxicity scores to be relatively low, 0.2591218315. Based on this threshold, I tested my hypothesis. With a sample of 12 comments, 11 of which were from the original CSV file, I tested whether a longer word in the comments, “hateful,” would be marked toxic less frequently than a shorter word in the comments, “hate.” I decided to include 6 comments with the word “hate” and 6 comments with the word “hateful.” Within these 6 comments, 3 comments were marked as toxic and 3 comments were marked as nontoxic. I decided to evenly separated toxic and nontoxic comments because it would allow for more accurate comparisons in toxicity scores. Through averaging toxicity scores within these trios, I found that although the “hate” toxic comments had the highest toxicity score average, 0.24825052666666667, this value still barely meets the aforementioned toxicity threshold score. Additionally, the nontoxic “hateful” comments, which should have had the lowest toxicity score average, had the second highest score at 0.22734073333333335. Therefore, because these values are fairly close together and do not depict a consistent trend, this test fails to support my hypothesis that Perspective API more frequently marks shorter words as toxic. A larger sample size might have allowed me to get more varied toxicity scores. Additionally, using a variety of synonyms of different lengths may have also made the test more comprehensive and accurate to the hypothesis.

